<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-11-24T20:34:45+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Keith Matthews</title><subtitle>This is a site I plan on using to showcase my work and projects im involved with.</subtitle><entry><title type="html">Break The Raid</title><link href="http://localhost:4000/2024/11/24/Break-the-Raid.html" rel="alternate" type="text/html" title="Break The Raid" /><published>2024-11-24T00:00:00+00:00</published><updated>2024-11-24T00:00:00+00:00</updated><id>http://localhost:4000/2024/11/24/Break-the-Raid</id><content type="html" xml:base="http://localhost:4000/2024/11/24/Break-the-Raid.html"><![CDATA[<h2 id="the-goal">The goal</h2>

<p>Get the raid large pools we created for proxmox on the hosts mss1 and mss2 broken out without breaking everything.</p>

<h2 id="why">Why?</h2>

<p>I would like to start using ceph inside the cluster and raid arrays don’t mix with ceph. I think setting up ceph will help as we scale up the cluster to make our storage more widely available. Also, one of our servers is setup with its operating system on a raid-0 which is not great.</p>

<h2 id="ok-so-how">OK, so how?</h2>

<p>We will start with mss2. things we will need to do include…</p>

<ul>
  <li>pull a dump of the config for the proxmox machine</li>
  <li>checkout <a href="https://pve.proxmox.com/wiki/Proxmox_Cluster_File_System_(pmxcfs)#_recovery">this-proxmox-wiki-article</a>
<img src="http://localhost:4000/images/Screenshot_20241031_123227_Chrome.jpg" alt="test" /></li>
  <li>migrate all virtual machines off of mss2 and onto mss1.</li>
  <li>remove all data from anything that currently is in ceph</li>
  <li>shutdown the OSB that lives on mss2 and remove it from ceph. ceph will complain about this. that’s OK. we will be re-adding stuff before too long.</li>
  <li>shut mss2 down. ensure you can still get to the idrac</li>
  <li>destroy the large raid in the virtual disk sub menu !!! YOU NEED TO MAKE SURE TO DESTROY THE CORRECT ONE OR THIS WILL BE AN ABSOLUTE PAIN !!!</li>
  <li>Do not switch the raid controller out of raid mode. the dell site about the raid controllers were using says it will pass the drives though in this mode while still controlling the raid for the operating system. I guess we’ll see if that is in fact true.</li>
  <li>After the raid is broken up, go to the physical disks tab and select non-RAID for the disks you want to pass though to the host system. per the Dell documents on the raid controller were using this should pass the drives straight into the system.</li>
  <li>now you get to see what happened :D</li>
  <li>boot up the host operating system and see what disks are there</li>
  <li>if the new drives show up in proxmox as target-able drives, add them as OSB’s</li>
  <li>assuming all this works the migration will be conducted in a similar fashion on mss1</li>
</ul>

<h2 id="ok-smart-guy-whats-the-backup-plan">OK Smart guy, whats the backup plan?</h2>

<p>A back out plan should not be necessary so long as no vm’s get lost and no data is lost. in the case the disks that used to be in the bulk raid array don’t show up, Ill likely have to get back into the raid card to sort it out. It is highly unlikely that proxmox gets wiped off the mss2 system since we will not be touching the raid that holds the disks with proxmox on them. the worst case here is that the drives will be passed though ONLY with the RAID card being setup in HBA mode. that would be a major bummer. we’ll probably want to decide if its worth the performance hit to go software raid for the OS and ceph for the rest of the drives. I have a feeling this cluster will never get big enough to justify that trade-off.</p>

<h2 id="extra-info">extra info</h2>

<p>We have a PERC H730 integrated Raid controllers in both dells here are the documents for them</p>

<p>On Mss2 we have a total of 8 drives with drive 0 and 1 setup in a raid 1 for the boot operating system. the remaining drives 2 - 7 are all 1TB and can be added to the ceph pool, they are currently in a raid 5</p>

<p>On mss1 we have a total of 8 drives with drive 0 and 1 setup in a raid 0 (this is not ideal) for the operating system. drives 2-7 are in a raid 5 and all have 1 TB of capacity</p>

<p>Crash only has 2 drives available. one for the operating system and one spare that’s currently in use by ceph. this is fine. It may be a good idea to balance the drives in the cluster a bit. we can do that later</p>

<p>mss1
<img src="http://localhost:4000/images/Screenshot_20241031_135634_Chrome.jpg" alt="test" /></p>

<p>mss2
<img src="http://localhost:4000/images/Screenshot_20241031_135627_Chrome.jpg" alt="test" /></p>

<h2 id="so-how-did-it-go-with-mss2">So how did it go with mss2?</h2>

<p>Well it ended up going well! A few things we learned…</p>

<ul>
  <li>when you turn off a Dell system using the ACPI shutdown (not like ripping the power out) the raid card and drives will be off also and you will not be able to manipulate them via the idrac interface.</li>
  <li>next we powered up the server and ended up kicking the OSB on mss2 off the Ceph cluster, and then ensuring that it was not mounted on the proxmox host. Once that was done we went back to the idrac and we’re able to delete the raided virtual disk. That fixed the 6 disks from the raid back to physical disks.</li>
  <li>next we were able to see the drives to non raid. We had to do this one by one with about 3-5 minutes in-between since there was some sort of job queue lock. As we added those drives they showed up on the proxmox system. This was all done while the proxmox servers was live.</li>
  <li>once all drives were added individually, we were able to add them to the Ceph cluster as OSB’s</li>
  <li>the one thing that kinda sucked there was we again had to add drives one by one. It is probable that if we add the ochestrator Ceph plugin we could add a bunch of drives at the same time but I’m not totally sure. I think I’m going to install that plugin and see about that for the next time</li>
  <li>we determined that we will likely need to re-install mss1 since the operating system currently lives on a raid 0</li>
</ul>

<h2 id="alright-so-how-do-we-do-the-thing-with-mss1">Alright so how do we do the thing with mss1?</h2>

<ul>
  <li>Reading online, it seems like you can convert a raid 0 to a raid 1 if you have the spare drives already avalible. What I can do to make this is is in this <a href="https://www.dell.com/community/en/conversations/poweredge-hddscsiraid/migrate-raid-0-to-raid-1-how/647f5019f4ccf8a8de8fb5c7">link</a>
<img src="http://localhost:4000/images/Screenshot_20241104_114028_Chrome.jpg" alt="test" /></li>
</ul>

<ol>
  <li>Migrate all Virtual machines off of mss1 (this was more dificult than strictly nessicary since a HA task was setup for zugzug. I turned it off (probably) by removing its uspace group temporarily)</li>
  <li>remove the OSB from the ceph pool that lives on MSS1</li>
  <li>Migrate/duplicate the metadata Managment service from mss1 to mss2</li>
  <li>Ensure the migration tasks have succeeded</li>
  <li>Check that you have serial access to he server via the idrac ( do have this! Its super cool I’ll write more later)</li>
  <li>shut down mss1</li>
  <li>install usb thumb drive for a live ubuntu desktop iso image. this is just a place holder so we can have all drives unmounted while the system is turned on. I may be able to do this with a virtual ISO which would be rad as hell</li>
  <li>Turn on the system using the idrac and boot into the thumb drive. Or virtual drive</li>
  <li>Access console via vnc to ensure your access and that all drives are disconnected.</li>
  <li>Once verified, get into the idrac again.</li>
  <li>Remove the raid 6 virtual disk, this will take a good couple minutes.</li>
  <li>Next you select the virtual disk (currently the raid 0 disks) and select reconfigure</li>
  <li>You will be prompted to add drives that are available to create a raid 1 and the data shoullllld transfer over</li>
  <li>The transfer will take a few hours.</li>
  <li>Check with your running live mounted system that disks have been copied over and reconfigured correctly.</li>
  <li>After that Ill need to take all drives that are unused and set them up as non raid disks. Will need to check with Garth if he cares about the boot disks being in specific slots.</li>
  <li>Once all reconfig tasks are done reboot the system and boot into the raid 1 virtual disk to confirm proxmox functionality.</li>
  <li>If that functions, the raid 0 virtual disk can be formated, wiped and unmounted from the proxmox os and you can break the raid in the idrac console. Once they are in non-raid mode, go ahead and add them to the Ceph pools</li>
</ol>

<ul>
  <li>
    <p>To get the serial console working I had to install the openManage Mobile app and the bVNC free application. Once those are installed, connect up to the idrac IP address and once connected select the virtual console. This will only work on mss1 since it has the enterprise level licence</p>
  </li>
  <li>
    <p>The one additional thing we need to be sure of is that there is a big ass wind storm kicking around Washington right now. That could make power less of a reliable factor. If power goes out during this operation were pretty much maxed out on the UPS so hopefully that wont happen? Gonna chat with Garth about that.</p>
  </li>
</ul>

<hr />

<p>So we paused the migration due to the wind storm that was happening on 11/4 and I went ahead with the migration on mss1 here’s how that went…</p>

<ul>
  <li>I had the machine still shut down from yesterday</li>
  <li>I used my Ubuntu with a GUI VM on mss2 to install the icedtea application to run the .jnlp files, interestingly while this worked on the VM in the micro-space, it did not work from my laptop. IDK if there are some firewall/port rules getting hit but something is preventing the virtual console working remotely. Not sure what’s going on there but there’s that. ( It did work from my phone so who knows. You cant load a virtual disk that way unfortunately though)</li>
  <li>I ended up starting with trying to load parted magic as a virtual disk before that became clear that I wouldn’t be able to do that.</li>
  <li>turns out the idrac wont let you configure the raid levels (as far as I can tell) so I needed to get into the bios</li>
  <li>This was done simply enough by setting the “next boot” option to the system bios and rebooting the server. This got me to the bios where I was able to eventually get to a bios configuration wizard (setup utility) and then go into the raid controller and then Virtual disks. This menu allowed me to reconfigure the raid0 array to a raid 1 array. There’s a problem with this though</li>
  <li>By adding drives to a raid 0 array to make it a raid 1 array, We have created a raid 10. This is NOT what we want. This is gonna mean that we have 4 whole disks taken up by the boot drive which kinda sucks. So how down we fix this?</li>
  <li>Well I think we have enough drives to create an entirely separate raid 1 array that will be our target.</li>
  <li>Then we will boot into a live system with all the raid arrays unmounted.</li>
  <li>we will have to resize the file system that proxmox lives so that it will fit on a 1  terabyte raid 1 array instead of the raid 10 array of 2 TB. Luckily only about 300 GB is used on the raid 10 array so this shouldn’t be a problem.</li>
  <li>We will use the DD command to copy over all the bytes from the raid 10 to the raid 1 array. This should mean that the raid 1 array becomes a boot-able proxmox volume</li>
  <li>once we verify that proxmox boots off the new raid 1 array Ill delete the raid 10 and add those drives into the Ceph pool</li>
  <li>If none of that works I’ll re-install all this stuff. That would kinda suck but that’s how it be sometimes.</li>
</ul>

<hr />

<h2 id="so-how-did-it-go-with-mss1">So how did it go with mss1?</h2>

<p>Not as well as I would have liked. There were two things I misunderstood. the first was how simply DD-ing the raid array from one drive to the other wouldn’t work well. Maybe there is a way I could have completed this but it would have involved messing with the grub configuration as well as additional changes. It is likely a worthwhile thing to try doing this on a test system in the future but at the time I was unable to complete things without wiping out all the drives.
the other issue was that when converting a raid 0 array to a raid 1 array using the dell idrac tools and the raid controller, there’s no way to change the array cleanly from one to the other. what it does instead is convert the whole array to a raid 10 which is a lot of drives and there isn’t enough of a performance bump for us to justify this. What I ended up doing was deleting all drives and starting fresh.
The nice thing here was that no critical data was lost. All virtual machines had been moved off of mss1 prior to starting the switch over and re-installing mss1 was a very simple task. all I really had to do was to make sure to set the IP address correct, set the root password the same as what we had previously and then re-add mss1 into the proxmox cluster.
There were some things I had to do to ceph to get it working correctly again, basically I uninstalled everything and started fresh with ceph. Now, we have 12 terabytes of ceph storage available accost the cluster. Crash really probably should have some drives moved onto it and we probably need to get some faster networking on that system to make the whole cluster act a bit better. load balancing small ceph clusters is critical and ours is somewhat poorly optimized at the moment.</p>

<p>In all I learned a whole bunch about getting access to the dell servers IDRAC interfaces, how to shuffle data around and I learned a bit about coming up with reasonable back-out plans. This whole episode really showed how  excellent proxmox’s clustering tools work.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[The goal]]></summary></entry><entry><title type="html">Logs</title><link href="http://localhost:4000/2024/11/23/Logs.html" rel="alternate" type="text/html" title="Logs" /><published>2024-11-23T00:00:00+00:00</published><updated>2024-11-23T00:00:00+00:00</updated><id>http://localhost:4000/2024/11/23/Logs</id><content type="html" xml:base="http://localhost:4000/2024/11/23/Logs.html"><![CDATA[<h2 id="goal">Goal</h2>

<p>As far as logging goes, I have largely left things to my fellows in the u-space. They have done some  excellent work with grafana and Prometheus to keep an eye on stats relevant to devices in our cluster. This includes scrolling logs for containers, network infrastructure, firewall events, and other types of logs. I find these logs to be very useful when it comes to tracing issues but not terribly useful for monitoring critical metrics such as ensuring updates, and checking in on disk utilization and system hardness.</p>

<p>This movement from running logs to a daily report for select users helps me keep an eye on selected systems and Identify issues with systems in a more active way. I think that this helps prevent alert fatigue if the alerts are only once a day and critical. I think of it like reading the news.</p>

<h2 id="checkout-the-code">Checkout the code!</h2>
<ul>
  <li><a href="https://github.com/kmatthews123/ansible/blob/main/Micro-Space/Playbooks/disk-utilization.yaml">File systems</a></li>
  <li><a href="https://github.com/kmatthews123/ansible/blob/main/Micro-Space/Playbooks/lynis-run.yaml">Lynis</a></li>
  <li>Ansible Auto Patching article <a href="https://kmatthews123.github.io/2024/09/02/Ansible-auto-patching.html">here</a></li>
</ul>

<h2 id="the-logs-im-targeting">The logs I’m targeting</h2>
<p>The three logs I’m  generating right now are as follows</p>
<ul>
  <li>File system usage</li>
  <li>hardening status</li>
  <li>update status
These three reports get pushed in different ways. For right now the File system usage and the hardening status reports get generated as needed. Ill probably move these into a once a week readout for review. I do want to use the hardening status report to inform setting up some more robust security settings on the servers we monitor. I also am sure that the file system usage can be monitored better by a smtp tool like cacti or grafana but I haven’t dug into how that works with alerting techniques like using webhooks to alert to issues with systems. For now here are my notes on these scripts.</li>
</ul>

<h3 id="file-system-usage">File system usage</h3>
<p>This ansible script uses the pydf tool to run a check on all host systems targeted by ansible. the goal here is to identify any outliers with root file systems that are either getting full unusual quickly or that may not have their full logical volume provisioned like a file system that is 80% full but only has the first 100Gib of a 500Gib disk set up for the file system to use. This helps me as a system admin identify systems that are incorrectly provisioned so I can get in and solve the problem before they fill up and become a problem for users. It also helps me get in front of storage issues like NFS volumes for docker containers that are getting too full.</p>

<p>The report for this command gets generated as an html document that encapsulates all systems. Im using the ansi2html python tool to convert all the readouts to a nice html file to be served.</p>

<p><img src="http://localhost:4000/images/Screenshot from 2024-11-23 11-42-55.png" alt="test" /></p>

<p>Truthfully this method is a bit of a hack. It relys on ansible.builtin.shell which is not the most clean way of running playbooks and is also probably less useful than active monitoring. This playbook is useful mostly for quick checks if you’ve got ansible up but no other monitoring.</p>

<h3 id="system-hardening">System hardening</h3>
<p>This playbook relies on the lynis software tool. I mostly plan on using this tool as a check on steps I take to harden servers using ansible. The other portion of its use will be to ensure systems managed by others but on systems I’m responsible for are adhering to a minimum level of security. I should probably look into ways to make the output a bit more brief for the security scan portion of this tool since I likely wont need all the recommended stuff that gets added and the links to remediation steps for when I run it against systems.</p>

<p><img src="http://localhost:4000/images/Screenshot from 2024-11-23 18-24-51.png" alt="test" /></p>

<p>This report usually gets generated as an ansi style message output by a command line. All I really do here is pipe it though a python tool to turn it into an html document with all the colors maintained.</p>

<h3 id="daily-updates">Daily updates</h3>
<p>This report is likely going to be tweaked a bit from my post on ansible updates. Mostly that change will be in a change on how the reports are generated. Currently, the combined log that gets pushed out informs the admin monitoring of updated packages and any systems that require reboots. While this is fine, my mentor pointed out to me that normally, the way these things are handled is that an alert will go to a communally monitored channel every day that things have gone well with whatever task. That task in this case is the update having run. If an admin wants more detail they can click that alert and be taken to a report. However, there is no action required. The other options are an alert that things did not go as expected and a relevant party should be notified that they need to attend to whatever did not go properly. The last option is that no alert is triggered. this is also relevant because if there is an alert that’s expected every day at 0900 but no alert is triggered, someone should probably go and investigate.</p>

<p><img src="http://localhost:4000/images/Screenshot from 2024-11-23 19-08-55.png" alt="test" /></p>

<p>Until I fix things I think this is a good general alert. It keeps me up to date and we don’t have as many folks with eyes on this infrastructure. I think that if this sort of tool gets implemented on the raspberry pi clubs proxmox cluster we will likely have to implement a more streamlined method.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Reboot Play</title><link href="http://localhost:4000/2024/11/22/Reboot-Play.html" rel="alternate" type="text/html" title="Reboot Play" /><published>2024-11-22T00:00:00+00:00</published><updated>2024-11-22T00:00:00+00:00</updated><id>http://localhost:4000/2024/11/22/Reboot-Play</id><content type="html" xml:base="http://localhost:4000/2024/11/22/Reboot-Play.html"><![CDATA[<h2 id="goal">Goal</h2>

<p>As part of my role maintaining systems in the microspace I created this playbook to automate the process of running daily updates against infrastructure and also sending notifications to a discord channel that I monitor. An important part of patching is updating Linux hosts to apply updated kernels.</p>

<h2 id="execution">Execution</h2>

<p>This playbook gets run manually by a system admin when they notice that one or several systems require a reboot. The playbook applies a serial restraint on the playbook to only execute reboots one system at a time with a 1 minute wait inbetween. While this probably doesn’t scale very well out to hundreds of systems, for now it works well to ensure systems hosting critical services in a swarm are not targeted all at once. Since the infrastructure Im keeping an eye on is also used by others in the u-space and eventually will also include services at the makerspace and for cascadeSTEAM Im planning for maintaining services as much as possible.</p>

<h2 id="the-code">The code</h2>

<p>The ansible script can be found <a href="https://github.com/kmatthews123/ansible/blob/main/Micro-Space/Playbooks/play-reboot-serial.yaml">here</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Goal]]></summary></entry><entry><title type="html">Ansible Auto Patching</title><link href="http://localhost:4000/2024/09/02/Ansible-auto-patching.html" rel="alternate" type="text/html" title="Ansible Auto Patching" /><published>2024-09-02T00:00:00+00:00</published><updated>2024-09-02T00:00:00+00:00</updated><id>http://localhost:4000/2024/09/02/Ansible-auto-patching</id><content type="html" xml:base="http://localhost:4000/2024/09/02/Ansible-auto-patching.html"><![CDATA[<h1 id="micro-space-ansible-setup">Micro Space Ansible Setup</h1>

<h2 id="goals">Goals</h2>

<ul>
  <li>Create inventory of devices present in the micro space in development and production clusters</li>
  <li>Run automated daily updates of development cluster of machines</li>
  <li>Run automated once a week updates of production machines</li>
  <li>Log information about updates</li>
  <li>notifications pushed to discord channel with stats on updates along with any required reboots of infrastructure</li>
</ul>

<h2 id="code">Code</h2>

<p>Checkout the code associated with this project!
<a href="https://github.com/kmatthews123/ansible">https://github.com/kmatthews123/ansible</a></p>

<h2 id="list-of-machines">list of machines</h2>

<h3 id="development">Development</h3>

<ul>
  <li>dev-nas-1 (planned)</li>
  <li>dev-compute-1</li>
  <li>dev-compute-2</li>
  <li>dev-gpu-compute-1</li>
</ul>

<h3 id="production">Production</h3>

<ul>
  <li>prod-nas-1</li>
  <li>prod-playbook-runner-compute-1</li>
  <li>prod-compute-2</li>
</ul>

<h3 id="network-devices">Network Devices</h3>

<ul>
  <li>mikrotik1</li>
</ul>

<h2 id="steps-to-complete">Steps to complete</h2>

<ul>
  <li>the following needs to be true on all machines
    <ul>
      <li>I need to be able to login using ssh</li>
      <li>The machine needs to have the borg user</li>
      <li>the borg user needs the following
        <ul>
          <li>No password sudo privileges</li>
          <li>ssh public keys for the machine that will run the ansible playbooks</li>
          <li>secure password (saved in Micro Space password vault)</li>
          <li>ssh daemon needs to be restarted to apply changes</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>While I directly working on any device, I fixed any minor configuration issues having to do with my access. on some machines I had set all sudo group members have no password access. this was an artifact of early experimentation with ansible on my part and was insecure. These problems have been rectified.</li>
</ul>

<hr />

<p>Now that the machines are all functional and able to talk together development of the ansible playbooks can begin.</p>

<ul>
  <li>
    <p>Decide on structure of file system for machine that will run ansible playbooks. that looks like this
<img src="http://localhost:4000/images/ansible-patching-tree.png" alt="tree output" /></p>
  </li>
  <li>Develop Playbooks for development and production clusters. these will largely be the same but with different variables plugged in. They need to be two separate playbooks due to the different timelines each is run under. Cron jobs will be used to deploy playbooks on their schedules.</li>
  <li>Build/use standard plays to run updates on remote Linux systems</li>
  <li>get Cron jobs working
    <ul>
      <li>Originally I was considering using a tool called semaphore UI primarily for the scheduling tools used to run ansible playbooks or other IAC tasks along with UI triggers and other things. this tool seems like the portainer of the ansible/terraform/opentofu world and while it looked neat and I will likely circle back to it at some point I decided that the management overhead for using this tool and getting it setup was more than I wanted to mess with at this point. Cron seems like plenty enough to run daily and weekly update/upgrade tasks. If I decide to move this all to semaphore-UI in the future there shouldn’t be a ton of additional work to make that happen.</li>
      <li>This was pretty straight forward. I followed a guide from phoenix systems to setup crontab. I set it up to run a simple playbook once every couple of minutes and was able to observe logs being generated.</li>
      <li>The server running the ansible playbooks is on UTC time so I set the Cron jobs to run at 10:00 UTC which is around 2-3 in the morning in the pacific timezone where these servers exist so the updates don’t interrupt others working in the micro space.</li>
      <li>The production machines will be at the same time but their Cron jobs will run on Sunday nights (this might change to Monday nights, need to talk with more experienced sys-admins about that)</li>
    </ul>
  </li>
</ul>

<h3 id="add-logging-functionality">Add Logging functionality</h3>

<ul>
  <li>the bulk of the playbooks to run these updates are tasks to properly complete the logging of the updates run on these servers.</li>
  <li>since all the servers run Debian the apt package manager was easy to setup and pull data from using the regex search function</li>
  <li>the playbooks register the output of the update step and the check if a reboot is required and starts to work with that data.
    <ol>
      <li>create a folder for the day the update runs in either the dev or prod folder</li>
      <li>save a .log file to the local host with just the hostname number of upgraded, newly installed, removed, and held packages, and if the reboot_required file exists.</li>
      <li>save a .vlog file to the local host with the entire standard output from ansible with the results of the update command.</li>
      <li>register the .log files saved to the local host to work with them via ansible</li>
      <li>echo the log file contents into a single file</li>
      <li>convert the log file into a string</li>
      <li>other processes are engaged. more on them shortly. after those steps are done the .log files get deleted but the .vlog files stay for later review of what was updated on remote systems since those packages are named.</li>
    </ol>
  </li>
  <li>It should be noted that the logging steps almost all happen on the machine that runs all the ansible playbooks. While not implemented, this will likely make it easy to ship these log files off to a NAS that can archive them or use them as data points for Grafana. that is out of the scope of this whole thing but will be worked on later.</li>
</ul>

<h3 id="add-notifications">Add notifications</h3>

<ul>
  <li>This was much more simple than I expected. Since the members of the micro space use discord for general communications I wanted to implement daily notifications of what systems got updated and how many updates got deployed along with the information on what servers are due for a reboot due to kernel updates.</li>
  <li>I setup a channel in the discord server and setup a simple webhook integration. copying over the link and then using the community.general.discord module allowed me to send the contents of the combined log file as an embedded message with all the info for all machines in the dev or production cluster. there is also some additional tweaks made to the simple webhook push to truncate the amount of characters (it seems there’s a limit of 1000 characters per message. this could be negated by building a discord bot but that is out of scope at this time.)</li>
  <li>This webhook solution kind of just works and didn’t need a ton in the way of configuration. the bulk of the challenge here was figuring out how to format the message and the logs in such a way that they were brief and to the point.</li>
</ul>

<h3 id="get-open-media-vault-working">Get Open Media Vault working</h3>

<ul>
  <li>This was largely the same as running updates for everything else but with a few caveats</li>
  <li>the main one was that the open media vault machine, while it contains the apt package manager, really relies on the <code class="language-plaintext highlighter-rouge">omv-upgrade</code> tool to run all necessary updates from the CLI or in an automated fashion. there may be an API but I don’t think ansible has been regularly used with open media vault so I kinda had to figure this out on my own</li>
  <li>What I figured out was that the <code class="language-plaintext highlighter-rouge">omv-upgrade</code> tool is used to upgrade the system and then <code class="language-plaintext highlighter-rouge">omv-salt deploy run --append-dirty</code> to apply the pending changes (that yellow banner that shows up at the top of the page any time you change something in open media vault)</li>
  <li>I could possibly setup the OMV7 steps to reside inside the play to update the regular Linux hosts but that would have been much more invasive and would likely have required I test against real hosts which I’m trying to avoid with the production network as much as possible.</li>
  <li>development for this portion was done on my local network targeting an OMV7 virtual machine</li>
</ul>

<h3 id="network-devices-1">Network devices</h3>

<ul>
  <li>I decided that with the progress other members of the micro space are making into logging with Grafana and specifically with that in conjunction with mikrotik devices, coupled with the fact that mikrotik devices are updated infrequently, I don’t think scheduled update and notification steps are necessary in the same way the Linux systems get daily package updates</li>
  <li>I do have updates of the mikrotik infrastructure using ansible working so I think that the workflow there will either be using Grafana to notify when updates are available for those pieces of infrastructure and then one of the admins responsible for that equipment will either just apply updates via the mikrotik dashboard or they can login to the ansible playbook machine and manually run the update playbooks. this will need to be a thing that is done with care because updating the mikrotik equipment brings down the network the Micro Space runs in for 5-10 minutes which is rude</li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>I learned a lot with this lab. I think that in the future, while all the logging stuff was/is very cool and does work, using tools like Prometheus and Grafana make more sense for alerting and there is likely a way to use those tools and their functions to more effectively do the logging and messaging than what I built here. That being said, I am really proud of this set of playbooks and I think it taught me a lot of valuable information that I can go on to use with ansible in the future and I got a better Idea of the bigger devops picture that exists. The update portion of this task is working and it is something that could be tied into with other sorts of tools down the line. running ansible playbooks from a dashboard based on different alerts to various sysadmins is a thing that is coming to the micro space.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Micro Space Ansible Setup]]></summary></entry><entry><title type="html">Ceph Ansible Lab</title><link href="http://localhost:4000/2024/08/20/Ceph-Ansible-Lab.html" rel="alternate" type="text/html" title="Ceph Ansible Lab" /><published>2024-08-20T00:00:00+00:00</published><updated>2024-08-20T00:00:00+00:00</updated><id>http://localhost:4000/2024/08/20/Ceph-Ansible-Lab</id><content type="html" xml:base="http://localhost:4000/2024/08/20/Ceph-Ansible-Lab.html"><![CDATA[<h3 id="goal">Goal:</h3>

<p>Follow along with a recent Network Chuck video on Ceph Clustering but use Ansible as much as possible to implement the lab. Follow along with his setup instructions but use ansible where possible to provision and configure machines.</p>

<h3 id="definitions">Definitions:</h3>

<p>Ceph: (Per <a href="https://ceph.io/en/">https://ceph.io/en/</a>) “Ceph is an open-source, distributed storage system”</p>

<p>Ansible: (Per <a href="https://www.ansible.com/">https://www.ansible.com/</a>) “Ansible is an open source IT automation engine that automates provisioning, configuration management, application deployment, orchestration, and many other IT processes. It is free to use, and the project benefits from the experience and intelligence of its thousands of contributors.”</p>

<h3 id="code">Code!</h3>

<p>Checkout the code I created and worked on to complete this project at my ansible github repo!</p>

<p><a href="https://github.com/kmatthews123/ansible">https://github.com/kmatthews123/ansible</a></p>

<h3 id="planned-steps-for-lab">Planned Steps for lab:</h3>

<ol>
  <li>
    <p>Provision Virtual Machines</p>

    <ol>
      <li>
        <p>follow the included spreadsheet</p>
      </li>
      <li>
        <p>install operating system to each machine per diagram, run updates</p>
      </li>
      <li>
        <p>setup default user on each machine and import GitHub ssh keys for that user, the keithm username with a simple password to get started and will not be online until ready to deploy new credentials via ansible. Use a secure pasword for the machine that will be running ansible playbooks against other hosts/</p>
      </li>
      <li>
        <p>setup host-names and IP addresses for each host.</p>
      </li>
      <li>
        <p>setup ssh connection between the machine that will be the ansible host (ubuntu-server6) and all the CEPH workers/ansible targets (this will require you turn on ssh password connection briefly. turn this back off when done)</p>
      </li>
      <li>
        <p>once all the basic provisioning is complete, turn off the 3 physical machines, and take a snapshot of the VM’s, these will be used while testing the ansible implementation to prevent unintended data loss</p>
      </li>
    </ol>
  </li>
</ol>

<table>
  <thead>
    <tr>
      <th>server NUM</th>
      <th>Host OS</th>
      <th>Host Hardware</th>
      <th>Hostname</th>
      <th>Ethernet Adapter</th>
      <th>IP address</th>
      <th>added storage</th>
      <th>roles</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>server 1:</strong></td>
      <td><strong>ubuntu 22.04</strong></td>
      <td>VM, 2GB RAM, 2 Core</td>
      <td>ubuntu-server1</td>
      <td>enp0s3</td>
      <td><strong>192.168.86.101</strong></td>
      <td>iscsi1, iscsi2</td>
      <td>ansible target, testing: ceph manager final: ceph worker</td>
    </tr>
    <tr>
      <td><strong>server 2:</strong></td>
      <td><strong>ubuntu 22.04</strong></td>
      <td>VM, 2 GB RAM, 2 Core</td>
      <td>ubuntu-server2</td>
      <td>enp0s3</td>
      <td><strong>192.168.86.102</strong></td>
      <td>iscsi3</td>
      <td>ansible target, ceph worker</td>
    </tr>
    <tr>
      <td><strong>server 3:</strong></td>
      <td><strong>ubuntu 22.04</strong></td>
      <td>Zimaboard 832</td>
      <td>ubuntu-server3</td>
      <td>enp0s2  enp0s3    (dhcp)</td>
      <td><strong>192.168.86.103</strong></td>
      <td>iscsi4, 500gb ssd, 500gb hdd</td>
      <td>ansible target, ceph manager/worker</td>
    </tr>
    <tr>
      <td><strong>server 4:</strong></td>
      <td><strong>ubuntu 20.04</strong></td>
      <td>Raspberry Pi4</td>
      <td>ubuntu-server4</td>
      <td>eth0</td>
      <td><strong>192.168.86.104</strong></td>
      <td>iscsi5</td>
      <td>ansible target, ceph worker</td>
    </tr>
    <tr>
      <td><strong>server 5:</strong></td>
      <td><strong>ubuntu 20.04</strong></td>
      <td>Raspberry Pi 3</td>
      <td>ubuntu-server5</td>
      <td>eth0</td>
      <td><strong>192.168.86.105</strong></td>
      <td>iscsi6</td>
      <td>ansible target, ceph worker</td>
    </tr>
    <tr>
      <td><strong>server 6:</strong></td>
      <td><strong>ubuntu 24.04</strong></td>
      <td>VM, 4 GB RAM, 4 Core</td>
      <td>ubuntu-server6</td>
      <td> </td>
      <td><strong>192.168.86.106</strong></td>
      <td>N/A</td>
      <td>ansible controller</td>
    </tr>
  </tbody>
</table>

<ol>
  <li>
    <p>provision ISCSI on NAS ( TrueNAS-scale ) per the below list (I don’t have any external drives to use for this project) ISCSI drives are sufficient to test with and adds the challenge of setting up the drives on each host.</p>

    <ol>
      <li>
        <p>iscsi1 - 200gb  VM1</p>
      </li>
      <li>
        <p>iscsi2 - 100gb VM1</p>
      </li>
      <li>
        <p>iscsi3 - 100gb Pi4</p>
      </li>
      <li>
        <p>iscsi4 - 100gb Pi3</p>
      </li>
      <li>
        <p>iscsi5 - 120gb VM2</p>
      </li>
      <li>
        <p>iscsi6 - 85gb VM2</p>
      </li>
    </ol>
  </li>
  <li>
    <p>setup HAPac2 Mikrotik router to get physical machines on the network, This is also going to be a useful tool to diagnose network issues as they occur. refer to lab network diagram for network layout. In my case the network is not ideal for lab work but its what I’ve got</p>
  </li>
  <li>
    <p>learn how to complete the following steps with ansible</p>

    <ol>
      <li>
        <p>run updates and upgrades</p>
      </li>
      <li>
        <p>setup a new user on all hosts with unique, strong passwords. delete bad password user.</p>
      </li>
      <li>
        <p>setup ssh as root (modify /etc/sshd.config) add root ssh key from ubuntu-server3 to all hosts (per network chuck ceph video though this is insecure as far as I know)</p>
      </li>
      <li>
        <p>mount iscsi drives on all systems</p>
      </li>
      <li>
        <p>prep drives (not the drive with the host os)</p>

        <ul>
          <li>
            <p>wipe out the disk (sgdisk –zap-all /dev/sd_ )</p>
          </li>
          <li>
            <p>wipe the file system (wipefs)</p>
          </li>
        </ul>
      </li>
      <li>
        <p>install dependent software on each system</p>

        <ul>
          <li>
            <p>docker</p>
          </li>
          <li>
            <p>lvm2 (should already be installed)</p>
          </li>
          <li>
            <p>open-iscsi (only needed in the case your using iscsi drives in the cluster)</p>
          </li>
          <li>
            <p>timedatectl status needs to be accurate (NTP synchronized and UTC time on all servers)</p>
          </li>
        </ul>
      </li>
      <li>
        <p>prep storage for ceph usage on all hosts</p>
      </li>
      <li>
        <p>set var for ceph_release</p>
      </li>
      <li>
        <p>install cephadm and make executeable after pulling down ceph repo to root user on</p>
      </li>
      <li>
        <p>cephadm bootstrap –mon-ip (ip address of manager)</p>
      </li>
      <li>
        <p>setup mon’s (these are monitors or managers of the cluster. you need a few of these for it to have proper redundancy)</p>
      </li>
      <li>
        <p>add osd’s (these are our drives essentially as I understand it)</p>
      </li>
    </ol>
  </li>
  <li>
    <p>once setup, play around with ceph, look at metrics, setup cephfs</p>
  </li>
</ol>

<h5 id="diagram-of-network">Diagram of network:</h5>

<p><img src="http://localhost:4000/images/ceph lab diagram.svg" alt="test" /></p>

<h3 id="results">Results:</h3>

<p>Overall I achieved my goals for this lab.</p>

<p>I think I got a better idea on using ansible to provision machines with software and setup basics stuff. I also learned about stuff that I will need to learn to get better at all of this.
those tools include:</p>

<ul>
  <li>
    <p>Key Management using Hashi-corp Vault, 1password, Bitwarden or Vaultwarden</p>
  </li>
  <li>
    <p>Cloud Init</p>
  </li>
  <li>
    <p>Teraform (IAC)</p>
  </li>
  <li>
    <p>Using ceph and understanding how it is used out in the wild.</p>
  </li>
  <li>
    <p>Virt-Manager for Linux based virtual machine management, specifically on remote hosts.</p>
  </li>
  <li>
    <p>understanding how ansible variables work and how they’re pulled into a playbook. I mostly understood this but there’s a lot more there to learn. Looks like there is a vs-code extension that could help with this called Ansible Variable Lookup though I haven’t used this yet.</p>
  </li>
</ul>

<p>A list of stuff I learned while completing this lab</p>

<ul>
  <li>
    <p>Setup TrueNAS served iscsi drives</p>
  </li>
  <li>
    <p>Using 1password CLI in tandem with bash scripting</p>
  </li>
  <li>
    <p>When a guide or documentation recommends opening up a root terminal before making a change there is probably a good reason for it</p>
  </li>
  <li>
    <p>PAM module and Yubi-key setup for a second factor on Linux</p>
  </li>
  <li>
    <p>Ansible</p>

    <ul>
      <li>
        <p>setup inventory files with hosts, host specific vars, and setting up nested group inventories</p>
      </li>
      <li>
        <p>using ansible vault files for playbook vars, calling specific vars in a loop, and other tasks for setting up users with secure credentials</p>
      </li>
      <li>
        <p>Ansible-Playbooks to:</p>

        <ul>
          <li>
            <p>run updates</p>
          </li>
          <li>
            <p>install software</p>
          </li>
          <li>
            <p>install docker</p>
          </li>
          <li>
            <p>install software that isn’t already in apts default repos</p>
          </li>
          <li>
            <p>set user time to all use UTC and ensure NTP time is enabled</p>
          </li>
          <li>
            <p>setup users with secure passwords that are stored at rest in encrypted vaults.</p>
          </li>
          <li>
            <p>create ssh keys and import them to all hosts so the ansible machine will always have access to those machines</p>
          </li>
          <li>
            <p>remove users that are on the systems</p>
          </li>
          <li>
            <p>pull in iscsi drives and mount them in the file system (this step the mount part specifically may have been what messed up the raspberry pi’s)</p>
          </li>
          <li>
            <p>format specific drives as specified by the host in the inventory file this could probably have been done better but it was towards the end of the project and I was just wanting to blast though.</p>
          </li>
          <li>
            <p>I messed around a bit with crowdsec, I think I have a good idea on what it does but it was distracting me from finishing the project so I put it to the side. Probably worth it to circle back on this.</p>
          </li>
          <li>
            <p>Install cephadm thought I think next time I will change the work flow to actually using cephadm from the machine that it is installed on That would have saved me a ton of time though the real answer is figure out ceph-ansible.</p>
          </li>
        </ul>
      </li>
      <li>
        <p>Setup ansible.cfg</p>
      </li>
      <li>
        <p>learn how to work with ansible-lint even though it sometimes doesn’t make much sense.</p>
      </li>
    </ul>
  </li>
  <li>
    <p>bash scripting</p>
  </li>
  <li>
    <p>gpg generally and as a part of a script</p>
  </li>
  <li>
    <p>documentation and pre-planing/diagramming a network</p>
  </li>
</ul>

<p>I looked into the ceph-ansible tool and I struggled for 3 days to understand how to use it. at that point I chose to simply follow along with the Network Chuck video when it came to the steps included in actualy installing ceph on all the prepared hosts. I think that I need to re-approach that particular task with fresh eyes in a month or two. The final results of all this setup was a working ceph cluster though with a few detraction’s. for some reason, even though I was able to confirm that the drives were wiped and had nothing on them and they should have been visible to the cluster, the two raspberry pi devices did not want to play nice. They were able to join the cluster but they were not able to share their drives with the cluster. I couldn’t figure this out and Im going to chock it up to while it may be possible to use raspberry pi’s with ceph it is likely very unstable and probably shouldn’t be attempted in anything resembling production. An additional problem was the pi 3 on average took about 2-3 times as long to run any given command. Once setup cephadm was fairly straight forward to use. I was able to use the cephadm command line tool to bring in other hosts and I was also able to do that task with the dashboard that was setup using cephadm. As I said in the things I learned I think that I should have thought of the flow of this lab as though I had two admins. one doing the ansible side of things and one responsible for administering ceph. this would have the ansible host setup all hosts and got them prepped to be managed by the ceph administrator. This way the ceph admin would get keys to login to the ceph machine where they could have done the administration from. These two people could defiantly be the same person filling two different roles but if I had thought of things like this the workflow would have made a lot more sense. below is a diagram of this changed workflow.</p>

<p><img src="http://localhost:4000/images/ceph_lab_diagram_post_lab_edit.png" alt="Ceph Lab post build" /></p>

<h3 id="things-to-do-next-time">Things to do next time:</h3>

<ul>
  <li>
    <p>setup remote hosts targeted by anisble via tail-scale</p>
  </li>
  <li>
    <p>scripts for setup should be applicable to different networks and built in a way that makes them scale-able</p>
  </li>
  <li>
    <p>use a key-management utility</p>
  </li>
  <li>
    <p>update and configure Mikrotik router using ansible</p>
  </li>
  <li>
    <p>use more hosts</p>
  </li>
  <li>
    <p>do not use raspberry pi’s for this</p>
  </li>
</ul>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Controller</title><link href="http://localhost:4000/2024/06/25/controller.html" rel="alternate" type="text/html" title="Controller" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>http://localhost:4000/2024/06/25/controller</id><content type="html" xml:base="http://localhost:4000/2024/06/25/controller.html"><![CDATA[<h2 id="custom-hid-device-design">Custom HID device design</h2>

<p>This project was focused on designing and building a custom controller for use at my job and to learn how to use PCB design software.</p>

<p><a href="https://github.com/kmatthews123/Controller">GitHub - kmatthews123/Controller: Controller designed by me with code from Joystick_XL library</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Custom HID device design This project was focused on designing and building a custom controller for use at my job and to learn how to use PCB design software. GitHub - kmatthews123/Controller: Controller designed by me with code from Joystick_XL library]]></summary></entry><entry><title type="html">Octodocker</title><link href="http://localhost:4000/2024/06/25/octodocker.html" rel="alternate" type="text/html" title="Octodocker" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>http://localhost:4000/2024/06/25/octodocker</id><content type="html" xml:base="http://localhost:4000/2024/06/25/octodocker.html"><![CDATA[<h2 id="why-use-docker-for-3d-printing">Why use docker for 3d printing?</h2>

<p>Several years ago, I was told about docker while commiserating with a freind about the lack of raspberry pi devices or at least their massivly blown up price during the pandemic. My friend pointed out docker and it opened a world up. after lots of hours watching youtube videos, digging into guides and setup documentation and poking at some basic containers using the single rasperry pi I had in my posession I started my project. At the time I had a few things already running on that pi and I was going to use it to also host octoprint to help me in managing my ender 3 pro. While this would have been a good enough use case and I will get into some of the things I had to figure out to make this single install work, it became even more important when I gained an additional 3 3d printers from a friend. these machines, to be easily operated and maintaind also needed octoprint running. this was the true benifit of running octoprint in a container. Because this application is really light, while it is easier to install directly to an sd card and run it bare metal on a raspberry pi both because it requries connecting to usb and because it greatly simplifies connection to a camera for streaming (more on that later). From what I understand you can hook more than one usb up to a pi running octoprint bare metal but I havent seen many people use this method. I assume it is because octoprint really seems to be designed to run one printer. One rasperry pi for one printer at a time when a medium spec pi 4 was hanging out in the 200$ range if you could find them at all was a really steap ask. In retrospect a Pi 3 would have been cheaper and have run just fine but again, more on that later. When I set to adding the additional 3 printers to my fleet of machines, I started with first converting my original octoprint container from running off of a docker “one liner” and building a docker compose file for it. This process was really straight forward because the docker hub page for the octoprint app has a compose file right there. The things I had to add and modify were easy enough for the first machine, simply probe for and pass though the usb device and also give the web portal its own defined port. This process would become more dificult as I duplicated the docker compose file for the usb pass though which I will get to in a bit. the ports being duplicated was easy enough and I made things simple by using 4 adjacent ports with numbers 1-4 in order of the printers. As far as the usb connections go, the printers, running all at the same time are not capable of saturating the usb bus, and the main problem was that as you plug things in and un-plug things, the linux operating system will change the port numbers for each individual device. This is an issue because the decleration for which usb device bets passed into the docker container is static. it cannot tell the diffrence between two diffrent usb inputs as far as I can tell. I have a feeling that I could possibly figure out how to script a way of polling the 3d printer for some kind of finger print at the time of spinning up the docker container to grab the right usb device automaticly but I havent figured that out yet. for now what that means is each /dev/ttyusb gets passed into its own container correctly by simply shutting down all containers, unpluging all usb devices from the host system, and then adding the devices and starting the containers that corespond with that printer 1 at a time. this way the usb inputs are enumerated correctly to give each instance of octoprint the correct machine. this is a pretty hacky way of doing things but unless multiple 3d printers get unpluged at the same time it is pretty functional once setup.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Why use docker for 3d printing?]]></summary></entry><entry><title type="html">Projects Im Involved With</title><link href="http://localhost:4000/2024/06/25/projects-Im-involved-with.html" rel="alternate" type="text/html" title="Projects Im Involved With" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>http://localhost:4000/2024/06/25/projects-Im-involved-with</id><content type="html" xml:base="http://localhost:4000/2024/06/25/projects-Im-involved-with.html"><![CDATA[<h4 id="checkout-projects-im-involved-in">Checkout projects I’m involved in</h4>

<p>Robo Ruckus, I have asssited with testing, and development of alternative robotics platforms. I have also run many public displays of the robo ruckus games
<a href="https://www.roboruckus.com/">Robo Ruckus</a></p>

<p>Big Boat Energy, This project I worked on with a freind in bellingham, we designed and built a boat platform that can follow waypoint missions with minimal intervention.
<a href="https://github.com/BigBoatEnergy/boat-robot">Boatymcboatface</a></p>

<p>Raspberry Pi Club Jeopardy game, I assisted this group of BTC students by coming up a plethora of linux themed jeopardy questions. I also assisted with a hardware implementation to make deploying the game at an event.
<a href="https://github.com/btc-raspberrypiclub/jeopardy">Jeopardy</a></p>

<p>Ruby based Discord bot. this discord bot acts as the frontend for a Large Language model and allows users in a discord channel to interact with an “AI”. I assisted by testing the bot, and also adding the “nice message” function
<a href="https://github.com/joshbuker/discord-bot">Ruby Bot</a></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Checkout projects I’m involved in Robo Ruckus, I have asssited with testing, and development of alternative robotics platforms. I have also run many public displays of the robo ruckus games Robo Ruckus Big Boat Energy, This project I worked on with a freind in bellingham, we designed and built a boat platform that can follow waypoint missions with minimal intervention. Boatymcboatface Raspberry Pi Club Jeopardy game, I assisted this group of BTC students by coming up a plethora of linux themed jeopardy questions. I also assisted with a hardware implementation to make deploying the game at an event. Jeopardy Ruby based Discord bot. this discord bot acts as the frontend for a Large Language model and allows users in a discord channel to interact with an “AI”. I assisted by testing the bot, and also adding the “nice message” function Ruby Bot]]></summary></entry><entry><title type="html">Solartv</title><link href="http://localhost:4000/2024/06/25/solartv.html" rel="alternate" type="text/html" title="Solartv" /><published>2024-06-25T00:00:00+00:00</published><updated>2024-06-25T00:00:00+00:00</updated><id>http://localhost:4000/2024/06/25/solartv</id><content type="html" xml:base="http://localhost:4000/2024/06/25/solartv.html"><![CDATA[<p>Fixing a solar tv</p>

<p>This is a brief test article to explain how I was able to “fix” a “solar tv” that would normally be sold in Africa as best as I can tell. This tv has a drm lockout feature that uses the designers 12-volt power supply from a solar array and solar charge controller as the key to unlock the drm. As best as I can tell, either the 12-volt power supply has some kind of signal in the power that the tv can decode or there is a different cable used to unlock the tv assuming you have paid the company. For multiple reasons this bothers me so I took a bit of time to figure out what was going on here and how I could defeat it. I have a feeling that these TVs were purchased on Ali-express or a similar kind of site as it does not seem these things are sold normally in the US. I did not purchase these TVs but inherited them from a makerspace I was involved with, the story I got is someone got them off the back of a truck at burning man.</p>

<p>what first tipped me off to the tv having drm in the first place is that when turning on two of the several of these things I have access to they both had some strange behavior where the backlight would be on during initial power up, and then it would start flickering at a regular interval before shutting off altogether. at first this behavior looked like the tv was damaged and I put it aside and evaluated a second tv. When this second device exhibited the exact same behavior, I got very suspicious. I was powering the devices via a regular 12-volt dc barrel jack and adapter off the wall power. I knew that I was supplying enough current since the tv draws something like twelve volt thirteen watts and my 12-volt brick was rated for five amps of twelve volt, I tried with a second power supply that was rated for a similar power level. At this point I stopped and started googling the company that makes these things. I had help with this from my friend and we discovered that the manufacturer sells these devices as part of a solar electricity kit (more likely an addon to the base kits) in Africa. They have a payment plan system that I think is used to lock out users from their equipment if they do not have a current code. Id imagine that this lockout device is what’s used to plug the tv into power via the double sided 12 volt cable that came included with this machine, and then it sends some kind of handshake to the tv when powerup happens to verify you aren’t using just some random car battery and your sticking to their ecosystem. This sort of lockout is scummy in my opinion and it makes these TVs e-waste for me since I did not have the lockout device and no means of getting one outside of contacting the company, so I decided to keep digging. It turns out that this 12-volt tv is a pretty common piece of gear you can order from overseas. They seem useful for things like digital signage and storefronts where quality matters less than a variety mounting and power options for something that is going to display the same three things for its entire life. When I found a picture of the circuit boards in this style of 12-volt TVs, I decided to pop mine open as well and find out what was different.</p>

<p>Apon opening the tv I discovered that it appeared the main difference was the addition of a black PCB with the manufacturers name that brought in the “Inverter” wires (it’s all 12 volt so I’m not sure what’s getting “inverted” but that’s what the main board had as a label for that plug) and output 4 wires (2 black and 2 red and tied into the same plug) that traveled into the TVs body. I had a feeling these were the power wires for the backlight, but I was able to confirm this with a multimeter. during initial power up, the four wires showed a 9.75 - 10.25 DC potential over those two wires, the multimeter was a cheap one and the flickering was quick so the LEDs using the full twelve volts is not a huge leap. that power dropped off in time with the backlight going off on the monitor, so I was quite sure those four wires were for the LED backlight. The next step was to try and figure out how to defeat the lockout chip to get those LEDs on normally.</p>

<p>At this point I started poking around the little driver board that was between the tv backlight and the mainboard. This little board had several surface mount chips with one programable logic chip as its center. I was able to read the name of that chip, but I do not know enough to pull its code off. I also was working with a limited set of tools so ultimately; I focused on what was bringing the power into the board instead of trying to force the board itself to do something other than design. I probed the incoming eight “Inverter” wires from the main board and found that there were two that were carrying twelve volts directly from the power supply. There are other wires that are on and off at different voltage levels and for various times, these are used to tell the board what to do about power when the tv is plugged in but “turned off” via a remote or power button. This is because the mainboard is always powered and is always putting twelve volts out to the breakout “drm” board. My quick solution was to simply plug some jumpers between the twelve volts into the board and the LED plug. this did work but it does come with the drawback of the TVs backlight is always on. This is undesirable for obvious reasons in an off-grid type setup using something like solar or batteries but for my use case it does not matter. There is a compelling reason to get back in there and design some custom circuitry that could act to shut the tv on and off based on signal from the IR receiver but that is a problem for another day.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Fixing a solar tv This is a brief test article to explain how I was able to “fix” a “solar tv” that would normally be sold in Africa as best as I can tell. This tv has a drm lockout feature that uses the designers 12-volt power supply from a solar array and solar charge controller as the key to unlock the drm. As best as I can tell, either the 12-volt power supply has some kind of signal in the power that the tv can decode or there is a different cable used to unlock the tv assuming you have paid the company. For multiple reasons this bothers me so I took a bit of time to figure out what was going on here and how I could defeat it. I have a feeling that these TVs were purchased on Ali-express or a similar kind of site as it does not seem these things are sold normally in the US. I did not purchase these TVs but inherited them from a makerspace I was involved with, the story I got is someone got them off the back of a truck at burning man. what first tipped me off to the tv having drm in the first place is that when turning on two of the several of these things I have access to they both had some strange behavior where the backlight would be on during initial power up, and then it would start flickering at a regular interval before shutting off altogether. at first this behavior looked like the tv was damaged and I put it aside and evaluated a second tv. When this second device exhibited the exact same behavior, I got very suspicious. I was powering the devices via a regular 12-volt dc barrel jack and adapter off the wall power. I knew that I was supplying enough current since the tv draws something like twelve volt thirteen watts and my 12-volt brick was rated for five amps of twelve volt, I tried with a second power supply that was rated for a similar power level. At this point I stopped and started googling the company that makes these things. I had help with this from my friend and we discovered that the manufacturer sells these devices as part of a solar electricity kit (more likely an addon to the base kits) in Africa. They have a payment plan system that I think is used to lock out users from their equipment if they do not have a current code. Id imagine that this lockout device is what’s used to plug the tv into power via the double sided 12 volt cable that came included with this machine, and then it sends some kind of handshake to the tv when powerup happens to verify you aren’t using just some random car battery and your sticking to their ecosystem. This sort of lockout is scummy in my opinion and it makes these TVs e-waste for me since I did not have the lockout device and no means of getting one outside of contacting the company, so I decided to keep digging. It turns out that this 12-volt tv is a pretty common piece of gear you can order from overseas. They seem useful for things like digital signage and storefronts where quality matters less than a variety mounting and power options for something that is going to display the same three things for its entire life. When I found a picture of the circuit boards in this style of 12-volt TVs, I decided to pop mine open as well and find out what was different. Apon opening the tv I discovered that it appeared the main difference was the addition of a black PCB with the manufacturers name that brought in the “Inverter” wires (it’s all 12 volt so I’m not sure what’s getting “inverted” but that’s what the main board had as a label for that plug) and output 4 wires (2 black and 2 red and tied into the same plug) that traveled into the TVs body. I had a feeling these were the power wires for the backlight, but I was able to confirm this with a multimeter. during initial power up, the four wires showed a 9.75 - 10.25 DC potential over those two wires, the multimeter was a cheap one and the flickering was quick so the LEDs using the full twelve volts is not a huge leap. that power dropped off in time with the backlight going off on the monitor, so I was quite sure those four wires were for the LED backlight. The next step was to try and figure out how to defeat the lockout chip to get those LEDs on normally. At this point I started poking around the little driver board that was between the tv backlight and the mainboard. This little board had several surface mount chips with one programable logic chip as its center. I was able to read the name of that chip, but I do not know enough to pull its code off. I also was working with a limited set of tools so ultimately; I focused on what was bringing the power into the board instead of trying to force the board itself to do something other than design. I probed the incoming eight “Inverter” wires from the main board and found that there were two that were carrying twelve volts directly from the power supply. There are other wires that are on and off at different voltage levels and for various times, these are used to tell the board what to do about power when the tv is plugged in but “turned off” via a remote or power button. This is because the mainboard is always powered and is always putting twelve volts out to the breakout “drm” board. My quick solution was to simply plug some jumpers between the twelve volts into the board and the LED plug. this did work but it does come with the drawback of the TVs backlight is always on. This is undesirable for obvious reasons in an off-grid type setup using something like solar or batteries but for my use case it does not matter. There is a compelling reason to get back in there and design some custom circuitry that could act to shut the tv on and off based on signal from the IR receiver but that is a problem for another day.]]></summary></entry></feed>